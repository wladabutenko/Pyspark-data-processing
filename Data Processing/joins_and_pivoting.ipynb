{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:43:14.831372300Z",
     "start_time": "2023-07-04T12:43:14.827621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:43:38.073596700Z",
     "start_time": "2023-07-04T12:43:34.525136600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Joins\n",
    "Merging different datasets is a very generic requirement present in most of data-processing pipelines in the big data world. PySpark offers a very convenient way to merge and pivot your dataframe values, as required."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:43:46.961569500Z",
     "start_time": "2023-07-04T12:43:46.961569500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"user_id\", StringType(), True), \\\n",
    "    StructField(\"country\", StringType(), True), \\\n",
    "    StructField(\"browser\", StringType(), True), \\\n",
    "    StructField(\"OS\", StringType(), True), \\\n",
    "    StructField(\"age\", IntegerType(), True) \\\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:43:58.838849Z",
     "start_time": "2023-07-04T12:43:58.838849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data2 = [(\"A203\",'India',\"Chrome\",\"WIN\", 33),\n",
    "         (\"A201\",'China',\"Safari\",\"MacOS\",35),\n",
    "         (\"A205\",'UK',\"Mozilla\",\"Linux\",25)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:44:12.442986400Z",
     "start_time": "2023-07-04T12:44:12.442986400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data2,schema=schema)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:44:54.015419100Z",
     "start_time": "2023-07-04T12:44:50.816285100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "region_data = spark.createDataFrame([('UK','Big London'),\n",
    "                                     ('China','Yunnan'),\n",
    "                                     ('India','Bihar')], schema=StructType() \\\n",
    "                                    .add(\"Country\",\"string\").add(\"Region\",\"string\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:50:54.746161500Z",
     "start_time": "2023-07-04T12:50:54.710957200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Country|    Region|\n",
      "+-------+----------+\n",
      "|     UK|Big London|\n",
      "|  China|    Yunnan|\n",
      "|  India|     Bihar|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:51:10.527688800Z",
     "start_time": "2023-07-04T12:51:03.702270400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    " new_df=df.join(region_data,on='Country')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:51:32.526252Z",
     "start_time": "2023-07-04T12:51:32.482608300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-----+---+----------+\n",
      "|country|user_id|browser|   OS|age|    Region|\n",
      "+-------+-------+-------+-----+---+----------+\n",
      "|  China|   A201| Safari|MacOS| 35|    Yunnan|\n",
      "|  India|   A203| Chrome|  WIN| 33|     Bihar|\n",
      "|     UK|   A205|Mozilla|Linux| 25|Big London|\n",
      "+-------+-------+-------+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:52:07.991166400Z",
     "start_time": "2023-07-04T12:51:57.852629400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pivoting\n",
    "We can use the pivot function in PySpark to simply create a pivot view of the dataframe for specific columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df=spark.read.options(delimiter=',', inferSchema='True', header='True').csv(\"data/Invistico_Airline.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:52:59.547842300Z",
     "start_time": "2023-07-04T12:52:58.411529Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+\n",
      "|Gender|Business|     Eco|Eco Plus|\n",
      "+------+--------+--------+--------+\n",
      "|Female|67308124|47526594| 7694611|\n",
      "|  Male|66823534|58843684| 9148861|\n",
      "+------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Gender').pivot('Class').sum('Flight Distance').fillna(0).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:53:56.459853500Z",
     "start_time": "2023-07-04T12:53:55.009367900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+\n",
      "|    Customer Type|      dissatisfied|         satisfied|\n",
      "+-----------------+------------------+------------------+\n",
      "|   Loyal Customer| 40.28649325768182|  42.1939376328628|\n",
      "|disloyal Customer|31.116924778761064|27.928070175438595|\n",
      "+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Customer Type').pivot('satisfaction').avg('Age').fillna(0).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-04T12:56:14.256400Z",
     "start_time": "2023-07-04T12:56:13.477160600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
